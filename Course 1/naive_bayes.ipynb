{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40a02b7-08aa-4812-bcef-fdf15fe86360",
   "metadata": {},
   "source": [
    "# Naive Bayes Sentiment Analysis\n",
    "This is even non-parametric meaning we just need to prepare the data and we're good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67981ca4-b783-4451-b52f-c20ba9bbf541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "343a9007-3cf9-448b-96e4-9f9ead5b8e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey', 'CDPR', 'When', 'do', 'we', 'get', 'Cyberpunk', '2088', 'FuckArasaka']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example use of tokenizer\n",
    "tweet_text = \"Hey! @CDPR! When do we get Cyberpunk 2088? #FuckArasaka\"\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokenizer.tokenize(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6de59e40-30fd-43f5-b84b-685b36d37cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'very', 'who', 'out', \"we're\", 'ourselves', \"they'll\", 'be', 'again', 'himself', 'some', 'during', 'own', 'when', 'your', 'having', 'over', 'should', \"isn't\", \"we'll\", \"shan't\", \"wouldn't\", 'each', \"he'll\", 'from', 'in', 'to', 'because', 'doing', 'off', 'these', \"i'd\", 'between', 'can', \"should've\", 'while', 'only', \"they'd\", 'were', 'nor', 'hasn', 'a', 'an', 'me', 'she', 'what', 'they', 't', 'there', 'with', 'yours', 'against', 'all', 'the', 'where', 'theirs', 'wouldn', \"hadn't\", 'or', 'i', 'once', 'his', 'weren', 'won', 'hadn', 'before', 'any', \"don't\", 'for', \"it's\", 'he', 'ma', 'shan', \"we've\", \"you've\", 'than', 'through', 'not', 'both', 'is', 'so', 've', 'this', 'here', 'whom', 'haven', \"mightn't\", \"i've\", 'him', 's', 'as', 'mightn', 'then', 'o', \"you'd\", 'how', \"i'm\", 'ours', 'why', 'that', 'down', \"she'd\", 'shouldn', 'does', \"doesn't\", \"aren't\", 'by', \"couldn't\", \"i'll\", 'now', 'do', 'and', \"he'd\", \"you're\", 'yourself', 'more', 'had', 'we', \"you'll\", 'other', 'aren', 'being', \"shouldn't\", \"that'll\", 'myself', \"haven't\", 'about', 'needn', 'couldn', 'her', 'wasn', \"won't\", \"we'd\", 'ain', 'same', \"they've\", 'our', \"weren't\", 'was', 'which', 'll', 'them', 'above', \"mustn't\", 'm', 'further', 'd', \"didn't\", 'if', 'too', 'herself', 'will', \"wasn't\", 'after', 'of', 'those', 'yourselves', 'just', 'am', 'has', 'at', 'have', 'did', \"he's\", 'itself', 're', 'themselves', 'up', 'don', 'their', 'are', 'but', 'it', 'y', 'you', 'didn', 'my', 'on', 'into', 'such', \"hasn't\", \"it'll\", 'mustn', 'its', 'below', 'under', 'until', 'doesn', 'isn', \"it'd\", 'hers', 'no', \"she'll\", 'few', \"they're\", 'been', 'most', \"needn't\", \"she's\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cheslaff/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4ac971b-a4ae-4b43-b9fd-6a6bda58a222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>morning</td>\n",
       "      <td>What a great day!!! Looks like dream.</td>\n",
       "      <td>positive</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>noon</td>\n",
       "      <td>I feel sorry, I miss you here in the sea beach</td>\n",
       "      <td>positive</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>night</td>\n",
       "      <td>Don't angry me</td>\n",
       "      <td>negative</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>We attend in the class just for listening teac...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>noon</td>\n",
       "      <td>Those who want to go, let them go</td>\n",
       "      <td>negative</td>\n",
       "      <td>Instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>night</td>\n",
       "      <td>According to , a quarter of families under six...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>morning</td>\n",
       "      <td>the plan to not spend money is not going well</td>\n",
       "      <td>negative</td>\n",
       "      <td>Instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>noon</td>\n",
       "      <td>uploading all my bamboozle pictures of facebook</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>night</td>\n",
       "      <td>congratulations ! you guys finish a month ear...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>morning</td>\n",
       "      <td>actually, I wish I was back in Tahoe.  I miss...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Instagram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Month  Day Time of Tweet  \\\n",
       "0    2018      8   18       morning   \n",
       "1    2018      8   18          noon   \n",
       "2    2017      8   18         night   \n",
       "3    2022      6    8       morning   \n",
       "4    2022      6    8          noon   \n",
       "..    ...    ...  ...           ...   \n",
       "494  2015     10   18         night   \n",
       "495  2021      2   25       morning   \n",
       "496  2022      5   30          noon   \n",
       "497  2018      8   10         night   \n",
       "498  2019      3   25       morning   \n",
       "\n",
       "                                                  text sentiment     Platform  \n",
       "0                What a great day!!! Looks like dream.  positive    Twitter    \n",
       "1       I feel sorry, I miss you here in the sea beach  positive    Facebook   \n",
       "2                                       Don't angry me  negative     Facebook  \n",
       "3    We attend in the class just for listening teac...  negative    Facebook   \n",
       "4                    Those who want to go, let them go  negative   Instagram   \n",
       "..                                                 ...       ...          ...  \n",
       "494  According to , a quarter of families under six...  negative     Twitter   \n",
       "495      the plan to not spend money is not going well  negative   Instagram   \n",
       "496    uploading all my bamboozle pictures of facebook   neutral    Facebook   \n",
       "497   congratulations ! you guys finish a month ear...  positive     Twitter   \n",
       "498   actually, I wish I was back in Tahoe.  I miss...  negative   Instagram   \n",
       "\n",
       "[499 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"sentiment_analysis.csv\")\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4be0b8da-fdfe-40c2-bbd7-1b576d7a0659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a great day!!! Looks like dream.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I feel sorry, I miss you here in the sea beach</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don't angry me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We attend in the class just for listening teac...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Those who want to go, let them go</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Sorry, we`ll try to keep it down.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>According to , a quarter of families under six...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>the plan to not spend money is not going well</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>congratulations ! you guys finish a month ear...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>actually, I wish I was back in Tahoe.  I miss...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text sentiment\n",
       "0                What a great day!!! Looks like dream.  positive\n",
       "1       I feel sorry, I miss you here in the sea beach  positive\n",
       "2                                       Don't angry me  negative\n",
       "3    We attend in the class just for listening teac...  negative\n",
       "4                    Those who want to go, let them go  negative\n",
       "..                                                 ...       ...\n",
       "492                  Sorry, we`ll try to keep it down.  negative\n",
       "494  According to , a quarter of families under six...  negative\n",
       "495      the plan to not spend money is not going well  negative\n",
       "497   congratulations ! you guys finish a month ear...  positive\n",
       "498   actually, I wish I was back in Tahoe.  I miss...  negative\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, remove all the other info\n",
    "data = dataframe[(dataframe[\"sentiment\"] == \"positive\") | (dataframe[\"sentiment\"] == \"negative\")][[\"text\", \"sentiment\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ec54a1d-059e-4ade-8343-f495551696be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    166\n",
       "negative    134\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad375b27-43b5-41bf-9a8d-cfed32acd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data[\"text\"])\n",
    "y = np.array(data[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "809fb172-69bd-491f-8ac7-a7e87dc7cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a great day!!! Looks like dream.\n",
      "['What', 'a', 'great', 'day', 'Looks', 'like', 'dream']\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "# Tokenize sentences\n",
    "for i in range(X.shape[0]):\n",
    "    X[i] = tokenizer.tokenize(X[i])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "187f0ff2-e272-437b-ba8b-0da933a04737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'a', 'great', 'day', 'Looks', 'like', 'dream']\n",
      "['great', 'day', 'Looks', 'like', 'dream']\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "print(X[0])\n",
    "for i in range(X.shape[0]):\n",
    "    X[i] = [w for w in X[i] if w.lower() not in stopwords_set]\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c71b3a7-0f4d-460c-8b0f-247453679e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'day', 'Looks', 'like', 'dream']\n",
      "['great', 'day', 'look', 'like', 'dream']\n"
     ]
    }
   ],
   "source": [
    "# Stem words\n",
    "stemmer = PorterStemmer()\n",
    "print(X[0])\n",
    "for i in range(X.shape[0]):\n",
    "    X[i] = [stemmer.stem(w) for w in X[i]]\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff92e405-9b1a-44d8-8561-814864e6b445",
   "metadata": {},
   "source": [
    "**We're done with simple preprocessing!**\n",
    "Time to form our dataset we'll be passing as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "018477b0-875c-4015-8284-890f860a7c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([int(lbl == \"positive\")for lbl in y])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "760245f4-3f2c-4b0e-86ae-bb0ec693f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sentence in X:\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1d71155-77c3-45ec-a9ed-e3bcbd1b00d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ce98590-d0ac-4374-a138-53443547aecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counts = {}\n",
    "negative_counts = {}\n",
    "\n",
    "for word in vocab:\n",
    "    for sentence, label in zip(X, y):\n",
    "        if word in sentence and label == 0:\n",
    "            negative_counts[word] = negative_counts.get(word, 1) + 1  # Laplacian smoothing\n",
    "        elif word in sentence and label == 1:\n",
    "            positive_counts[word] = positive_counts.get(word, 1) + 1  # Same here\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in positive_counts:\n",
    "        positive_counts[word] = 1  # And here it's also laplacian smoothing\n",
    "    elif word not in negative_counts:\n",
    "        negative_counts[word] = 1\n",
    "    \n",
    "positive_counts[\"cute\"], negative_counts[\"cute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "07817bff-fd6e-4a9d-9ccd-fb1c11bfa8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925, 1680)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sum = sum([v for v in positive_counts.values()])\n",
    "negative_sum = sum([v for v in negative_counts.values()])\n",
    "positive_sum, negative_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4cd8d9e3-29df-4e39-9dcf-1a58855b827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_probas = {}\n",
    "negative_probas = {}\n",
    "for word in vocab:\n",
    "    positive_probas[word] = positive_counts[word] / positive_sum\n",
    "    negative_probas[word] = negative_counts[word] / negative_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d9166f0-f6ea-4079-9f03-ce01bbc25004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0015584415584415584, 0.0005952380952380953)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_probas[\"cute\"], negative_probas[\"cute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c3dfc84f-22c8-43ec-ae34-9db35e364e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, this is it!\n",
    "# No training for this one\n",
    "# Let's test it out!\n",
    "\n",
    "def predict(sentence):\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    cleaned = [w for w in tokenized if w.lower() not in stopwords_set]\n",
    "    stemmed = [stemmer.stem(w) for w in cleaned]\n",
    "    log_prior = np.log(166 / 134)  # Sorry for hardcoding it, but we need it since the data is a bit skewed\n",
    "    score = 0.0\n",
    "    for word in stemmed:\n",
    "        if word not in vocab:\n",
    "            continue  # unknown word is considered neutral and adds nothing to the score\n",
    "        score += np.log(positive_probas[word] / negative_probas[word])\n",
    "    print(score)\n",
    "    print(\"Positive\" if score > 0.0 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d2fd93e0-c4b9-4ff7-9752-54b196a06958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.655627294903475\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "predict(\"Hello there people! :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cd27b8a9-c9bc-4509-9d58-73e979427deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.790423157647622\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "predict(\"Sad depression sad! :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f5470-2b6b-492d-9830-8ce04dc6eafa",
   "metadata": {},
   "source": [
    "### The higher the score is the more positive sentiment is\n",
    "### The less the score is the more negative sentiment is\n",
    "### Score of 0 is neutral\n",
    "But I think you know all these details if you're going through the specialization like I do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3393a6aa-b8ef-445d-bffb-c4f5104cb81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5570150062353653\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "predict(\"Deez Nuts\")  # Ceritified brainrot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149cc078-b963-4dd7-8af2-8b177cdb19ac",
   "metadata": {},
   "source": [
    "# Looks Like it works as expected\n",
    "(Notice: It's a very very very very very very dirty baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
