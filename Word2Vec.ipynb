{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "nomenclature:\n",
        "- target - word in context window\n",
        "- context - given word (input)"
      ],
      "metadata": {
        "id": "G4ZpT0qs6CCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "-0k7QhnHK2Mx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FETi5yBH6l9N",
        "outputId": "4f0a7936-357a-4600-9142-26394661eec9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1. Data Preparation"
      ],
      "metadata": {
        "id": "1PVBsXOx4Hbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "with open(\"data.txt\", \"r\") as f:\n",
        "  corpus = f.read().lower()\n",
        "  corpus = tokenizer.tokenize(corpus)\n",
        "  vocab = sorted(set(corpus))\n",
        "  vocab_size = len(vocab)\n",
        "  encoded_vocab = torch.tensor(range(vocab_size))  # used for one hot encoding"
      ],
      "metadata": {
        "id": "inGatCSr4Ft9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Preparation\n",
        "EMB_DIM = 150\n",
        "context_emb = torch.randn(vocab_size, EMB_DIM).to(device)\n",
        "target_emb = torch.randn(vocab_size, EMB_DIM).to(device)\n",
        "vocab_ohe = F.one_hot(encoded_vocab, vocab_size).to(device).float()\n",
        "\n",
        "context_emb.shape, target_emb.shape, vocab_ohe.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHqG4T-H5SIN",
        "outputId": "91367369-e2c3-46d1-981d-50112a0947b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([11456, 150]),\n",
              " torch.Size([11456, 150]),\n",
              " torch.Size([11456, 11456]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_emb.requires_grad_(True)\n",
        "target_emb.requires_grad_(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu5cGt4UFJc5",
        "outputId": "feffa6fd-6cd3-45af-e554-3cdb33faf8c2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.7779e-01, -2.6384e-01, -5.7726e-01,  ...,  1.3826e+00,\n",
              "         -6.3111e-01, -1.2829e+00],\n",
              "        [-1.3845e+00,  1.0692e+00, -3.8071e-01,  ..., -1.6524e+00,\n",
              "         -1.2421e+00, -6.8716e-01],\n",
              "        [ 7.9428e-02, -1.8727e-03, -1.2963e+00,  ...,  6.6184e-01,\n",
              "          1.2117e+00, -1.8842e-01],\n",
              "        ...,\n",
              "        [ 7.5691e-01,  7.6872e-01,  1.2065e-01,  ...,  1.0301e+00,\n",
              "          1.2501e+00,  1.8313e+00],\n",
              "        [-2.9517e-02,  2.0976e-01, -4.2028e-01,  ...,  4.1318e-01,\n",
              "          7.7671e-01, -5.0845e-01],\n",
              "        [ 2.1846e+00, -2.0764e+00, -1.4999e-01,  ..., -7.8781e-02,\n",
              "          4.4944e-01, -5.5593e-01]], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2. Skip-gram"
      ],
      "metadata": {
        "id": "eUmgjmET6wuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.NLLLoss()\n",
        "optimizer = optim.Adam([context_emb, target_emb], lr=0.001)"
      ],
      "metadata": {
        "id": "kp7LxOCoAoBd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 2\n",
        "\n",
        "for epoch in range(1):\n",
        "  for context_i in tqdm(range(WINDOW_SIZE, len(corpus)-WINDOW_SIZE-1)):\n",
        "    prev_targets = [vocab.index(w) for w in corpus[context_i-WINDOW_SIZE:context_i]]\n",
        "    past_targets = [vocab.index(w) for w in corpus[context_i+1:context_i+WINDOW_SIZE+1]]\n",
        "\n",
        "    context = torch.tensor(vocab.index(corpus[context_i]))\n",
        "    targets = torch.tensor(prev_targets + past_targets).to(device)\n",
        "\n",
        "    ohe_context = vocab_ohe[context].unsqueeze(0)\n",
        "    context_word_emb = ohe_context @ context_emb\n",
        "    probability = torch.softmax(context_word_emb @ target_emb.T, dim=1)  # Over all vocab\n",
        "    loss = 0\n",
        "    for target in targets:\n",
        "      loss += loss_fn(probability, target.unsqueeze(0))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LZS3AOi6jVG",
        "outputId": "d1dd224d-66e9-434c-b270-c223493b7825"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208525/208525 [08:34<00:00, 405.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCxRCxU88sAK"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}